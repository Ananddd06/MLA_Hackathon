# -*- coding: utf-8 -*-
"""MLA_Hackathon.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m-qLD_x-zuphsoxjjkYZwbdzamAU09GX
"""

import pandas as pd

# Load the datasets
train_df = pd.read_csv('/content/Train_set.csv')
test_df = pd.read_csv('/content/Test_set.csv')

# Display the first 5 rows
print("Train DataFrame head:")
display(train_df.head())
print("\nTest DataFrame head:")
display(test_df.head())

# Print the shape of the DataFrames
print("\nTrain DataFrame shape:", train_df.shape)
print("Test DataFrame shape:", test_df.shape)

# Display data types and non-null values
print("\nTrain DataFrame info:")
train_df.info()
print("\nTest DataFrame info:")
test_df.info()

# Check for duplicate rows
print("\nTrain DataFrame duplicate rows:", train_df.duplicated().sum())
print("Test DataFrame duplicate rows:", test_df.duplicated().sum())

# Calculate and display missing values
print("\nTrain DataFrame missing values:")
print(train_df.isnull().sum())
print("\nTest DataFrame missing values:")
print(test_df.isnull().sum())

"""## Exploratory data analysis (eda)

### Subtask:
Analyze categorical features, visualize missing values and outliers, examine the target variable distribution, and visualize correlations among numerical features. Handle skewness if necessary.

**Reasoning**:
Identify categorical and numerical columns, analyze unique values and frequencies for categorical columns, and visualize missing values for both train and test datasets.
"""

train_df['default'].value_counts()

test_df.shape

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Identify categorical and numerical columns
categorical_cols_train = train_df.select_dtypes(include='object').columns
numerical_cols_train = train_df.select_dtypes(include=np.number).columns

categorical_cols_test = test_df.select_dtypes(include='object').columns
numerical_cols_test = test_df.select_dtypes(include=np.number).columns

print("Categorical columns in train data:", categorical_cols_train.tolist())
print("Numerical columns in train data:", numerical_cols_train.tolist())
print("\nCategorical columns in test data:", categorical_cols_test.tolist())
print("Numerical columns in test data:", numerical_cols_test.tolist())

# Analyze unique values and frequencies for categorical columns in train data
print("\nAnalysis of categorical columns in train data:")
for col in categorical_cols_train:
    print(f"\nColumn: {col}")
    print("Unique values:", train_df[col].unique())
    print("Value Counts:\n", train_df[col].value_counts())

# Analyze unique values and frequencies for categorical columns in test data
print("\nAnalysis of categorical columns in test data:")
for col in categorical_cols_test:
    print(f"\nColumn: {col}")
    print("Unique values:", test_df[col].unique())
    print("Value Counts:\n", test_df[col].value_counts())


# Visualize missing values
plt.figure(figsize=(15, 6))

plt.subplot(1, 2, 1)
sns.heatmap(train_df.isnull(), cbar=False, cmap='viridis')
plt.title('Missing Values Heatmap (Train Data)')

plt.subplot(1, 2, 2)
sns.heatmap(test_df.isnull(), cbar=False, cmap='viridis')
plt.title('Missing Values Heatmap (Test Data)')

plt.tight_layout()
plt.show()

"""**Reasoning**:
Visualize the distribution of numerical features using histograms or box plots to detect outliers and understand their spread, analyze the target variable distribution, calculate and visualize the correlation matrix of numerical features, and check for skewness in numerical features.


"""

# Visualize distributions of numerical features using box plots to detect outliers
numerical_cols_to_plot = numerical_cols_train.drop(['ID', 'default']) # Exclude ID and target

n_cols = 4
n_rows = (len(numerical_cols_to_plot) + n_cols - 1) // n_cols

plt.figure(figsize=(15, n_rows * 5))

for i, col in enumerate(numerical_cols_to_plot):
    plt.subplot(n_rows, n_cols, i + 1)
    sns.boxplot(y=train_df[col])
    plt.title(f'Box Plot of {col}')
    plt.ylabel(col)

plt.tight_layout()
plt.show()

# Analyze the distribution of the target variable ('default') in the training dataset
plt.figure(figsize=(6, 4))
sns.countplot(x='default', data=train_df)
plt.title('Distribution of Target Variable (default)')
plt.xlabel('Default Status')
plt.ylabel('Count')
plt.show()

# Calculate and visualize the correlation matrix of numerical features in the training data
correlation_matrix = train_df[numerical_cols_train].corr()

plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Matrix of Numerical Features (Train Data)')
plt.show()

# Check for skewness in numerical features
skewness = train_df[numerical_cols_train].skew()
print("\nSkewness of numerical features in train data:")
print(skewness)

"""## Data preprocessing

### Subtask:
Handle missing values, encode categorical features, scale numerical features, and remove outliers. Save the scaler for later use.

**Reasoning**:
Handle missing values using imputation and create a new category for missing categorical values, encode categorical features using one-hot encoding, scale numerical features using StandardScaler and save the scaler, and handle outliers using clipping.
"""

!pip install category_encoders

"""**Reasoning**:
The previous attempt failed due to a missing library. I need to correct the code to use available libraries and ensure the preprocessing steps (handling missing values, encoding, scaling, outlier handling) are correctly applied. I will use StandardScaler and OneHotEncoder from sklearn, and SimpleImputer for missing values. I will also use clipping for outlier handling as planned.


"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler
from category_encoders import TargetEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
import joblib
import warnings
warnings.filterwarnings("ignore")

# === 1Ô∏è‚É£ Define feature groups ===
label_encoded_features = ['loan_term', 'loan_grade', 'loan_subgrade', 'job_experience', 'application_type']
onehot_encoded_features = ['home_ownership', 'income_verification_status', 'loan_purpose']
target_encoded_features = ['state_code']

# Identify numerical features (excluding ID and target variable)
numerical_features = train_df.select_dtypes(include=np.number).columns.tolist()
numerical_features.remove('ID')
numerical_features.remove('default')

# === 2Ô∏è‚É£ Remove rows with missing target values ===
train_df = train_df.dropna(subset=['default'])

# === 3Ô∏è‚É£ Split data ===
X_train = train_df.drop('default', axis=1)
y_train = train_df['default'].astype(int)
X_test = test_df.copy()

# === 4Ô∏è‚É£ Define transformers ===
numerical_transformer = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

label_transformer = Pipeline([
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))
])

onehot_transformer = Pipeline([
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
])

target_transformer = Pipeline([
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('target', TargetEncoder(handle_unknown='value', handle_missing='value'))
])

# === 5Ô∏è‚É£ Combine all transformers ===
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('label', label_transformer, label_encoded_features),
        ('onehot', onehot_transformer, onehot_encoded_features),
        ('target', target_transformer, target_encoded_features)
    ],
    remainder='drop'
)

# === 6Ô∏è‚É£ Fit & transform ===
X_train_preprocessed = preprocessor.fit_transform(X_train, y_train)
X_test_preprocessed = preprocessor.transform(X_test)

# === 7Ô∏è‚É£ Get feature names ===
num_feature_names = numerical_features
label_feature_names = label_encoded_features
onehot_feature_names = preprocessor.named_transformers_['onehot'].named_steps['onehot'].get_feature_names_out(onehot_encoded_features)
target_feature_names = target_encoded_features
all_feature_names = list(num_feature_names) + list(label_feature_names) + list(onehot_feature_names) + list(target_feature_names)

# === 8Ô∏è‚É£ Convert to DataFrame ===
X_train_preprocessed_df = pd.DataFrame(X_train_preprocessed, columns=all_feature_names)
X_test_preprocessed_df = pd.DataFrame(X_test_preprocessed, columns=all_feature_names)

print(f"‚úÖ Preprocessing completed!\nTrain shape: {X_train_preprocessed_df.shape}\nTest shape: {X_test_preprocessed_df.shape}")

# === 9Ô∏è‚É£ Save pipeline ===
joblib.dump(preprocessor, "preprocessor_pipeline.pkl")
print("üß† Saved preprocessing pipeline as 'preprocessor_pipeline.pkl'")

X_test.shape

X_test_preprocessed.shape

# from imblearn.combine import SMOTEENN
# from collections import Counter

# # === 1Ô∏è‚É£ Check original distribution ===
# print("üìä Original class distribution:", Counter(y_train))

# # === 2Ô∏è‚É£ Apply SMOTEENN ===
# smote_enn = SMOTEENN(random_state=42)
# X_resampled, y_resampled = smote_enn.fit_resample(X_train_preprocessed_df, y_train)

# # === 3Ô∏è‚É£ Check new distribution ===
# print("‚úÖ After SMOTEENN balancing:", Counter(y_resampled))

# # === 4Ô∏è‚É£ Convert back to DataFrame (for easy inspection or saving) ===
# X_resampled_df = pd.DataFrame(X_resampled, columns=X_train_preprocessed_df.columns)
# y_resampled = pd.Series(y_resampled, name='default')

# print(f"\nüîπ Resampled X shape: {X_resampled_df.shape}")
# print(f"üîπ Resampled y shape: {y_resampled.shape}")

X_train.shape, X_test.shape

"""## Model Building

### Subtask:
Split train data into X_train, X_test, y_train, y_test. Train multiple classification models and evaluate using Accuracy, Precision, Recall, F1-score, and Confusion Matrix. Compare results and choose the best-performing model.

**Reasoning**:
Split the preprocessed training data into training and validation sets, train different classification models, evaluate each model using specified metrics, and store the results in a DataFrame for comparison.
"""

!pip install catboost

from sklearn.model_selection import train_test_split

X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(
    X_train_preprocessed_df, y_train,
    test_size=0.2,
    random_state=42,
    stratify=y_train  # ‚úÖ Stratify after resampling
)

# Show the shapes and balance
print("‚úÖ Data Shapes After Resampling and Split:")
print(f"X_train_split: {X_train_split.shape}")
print(f"X_val_split:   {X_val_split.shape}")
print(f"y_train_split: {y_train_split.shape}")
print(f"y_val_split:   {y_val_split.shape}")

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import (
    RandomForestClassifier, GradientBoostingClassifier,
    ExtraTreesClassifier, AdaBoostClassifier
)
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import pandas as pd
import time

# === 1Ô∏è‚É£ Split the balanced (resampled) data ===
X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(
    X_train_preprocessed_df, y_train,
    test_size=0.2,
    random_state=42,
    stratify=y_train  # ‚úÖ Stratify after resampling
)
print("‚úÖ Data Shapes After Resampling and Split:")
print(f"X_train_split: {X_train_split.shape}")
print(f"X_val_split:   {X_val_split.shape}")
print(f"y_train_split: {y_train_split.shape}")
print(f"y_val_split:   {y_val_split.shape}")


# === 2Ô∏è‚É£ Initialize all models (CPU only) ===
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Random Forest": RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1),
    "XGBoost (CPU)": XGBClassifier(
        tree_method='hist',          # efficient CPU method
        predictor='cpu_predictor',   # ensure CPU usage
        eval_metric='logloss',
        random_state=42,
        n_jobs=-1
    ),
    "LightGBM (CPU)": LGBMClassifier(
        device_type='cpu',           # force CPU
        random_state=42,
        n_jobs=-1
    ),
    "CatBoost (CPU)": CatBoostClassifier(
        task_type='CPU',             # force CPU
        verbose=0,
        random_state=42
    ),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42),
    "Extra Trees": ExtraTreesClassifier(n_estimators=200, random_state=42, n_jobs=-1),
    "AdaBoost": AdaBoostClassifier(random_state=42),
    "KNN": KNeighborsClassifier(n_jobs=-1)
}

# === 3Ô∏è‚É£ Train and evaluate all models ===
results = []
for name, model in models.items():
    print(f"\nüöÄ Training {name}...")
    start = time.time()
    model.fit(X_train_split, y_train_split)
    end = time.time()

    y_pred = model.predict(X_val_split)

    accuracy = accuracy_score(y_val_split, y_pred)
    precision = precision_score(y_val_split, y_pred)
    recall = recall_score(y_val_split, y_pred)
    f1 = f1_score(y_val_split, y_pred)
    cm = confusion_matrix(y_val_split, y_pred)

    results.append({
        "Model": name,
        "Accuracy": round(accuracy, 4),
        "Precision": round(precision, 4),
        "Recall": round(recall, 4),
        "F1-score": round(f1, 4),
        "Train Time (min)": round((end - start) / 60, 2),
        "Confusion Matrix": cm.tolist()
    })

    print(f"‚úÖ {name} - Acc: {accuracy:.4f}, Prec: {precision:.4f}, Rec: {recall:.4f}, F1: {f1:.4f}")
    print(f"‚è±Ô∏è Time: {(end - start)/60:.2f} min")

# === 4Ô∏è‚É£ Display results ===
results_df = pd.DataFrame(results)
print("\nüìä Model Comparison (Sorted by F1-score):")
results_df = results_df.sort_values(by="F1-score", ascending=False).reset_index(drop=True)
display(results_df)

import pandas as pd
import time
import joblib
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from catboost import CatBoostClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.ensemble import GradientBoostingClassifier

# ======================================
# ‚öôÔ∏è Step 1: Data Info
# ======================================
print(f"‚úÖ X_train_split shape: {X_train_split.shape}")
print(f"‚úÖ y_train_split shape: {y_train_split.shape}")
print(f"‚úÖ X_val_split shape: {X_val_split.shape}")
print(f"‚úÖ y_val_split shape: {y_val_split.shape}")

print(f"\nüìä Class Distribution in Train Set:")
print(y_train_split.value_counts().to_dict())

# ======================================
# üß† Step 2: Define Models (CPU-only)
# ======================================
models = {
    "CatBoost": CatBoostClassifier(task_type='CPU', verbose=0, random_state=42),
    "XGBoost": XGBClassifier(
        tree_method='hist',
        predictor='cpu_predictor',
        eval_metric='logloss',
        random_state=42,
        use_label_encoder=False
    ),
    "LightGBM": LGBMClassifier(device_type='cpu', random_state=42, n_jobs=-1),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42)
}

# ======================================
# üéØ Step 3: Parameter Grids
# ======================================
param_grids = {
    "CatBoost": {
        'depth': [6, 8, 10],
        'learning_rate': [0.03, 0.05, 0.1],
        'iterations': [300, 500, 700],
        'l2_leaf_reg': [3, 5, 7]
    },
    "XGBoost": {
        'n_estimators': [200, 400, 600],
        'learning_rate': [0.03, 0.05, 0.1],
        'max_depth': [4, 6, 8],
        'subsample': [0.8, 1.0],
        'colsample_bytree': [0.8, 1.0]
    },
    "LightGBM": {
        'n_estimators': [200, 400, 600],
        'learning_rate': [0.03, 0.05, 0.1],
        'num_leaves': [31, 63, 127],
        'max_depth': [-1, 10, 15],
        'subsample': [0.8, 1.0],
        'colsample_bytree': [0.8, 1.0]
    },
    "Gradient Boosting": {
        'n_estimators': [100, 200, 300],
        'learning_rate': [0.03, 0.05, 0.1],
        'max_depth': [3, 5],
        'subsample': [0.8, 1.0],
        'min_samples_split': [2, 5]
    }
}

# ======================================
# üöÄ Step 4: RandomizedSearchCV (Tuning)
# ======================================
best_models = {}
results = []

for model_name, model in models.items():
    print(f"\nüîç Tuning {model_name}...")
    param_grid = param_grids[model_name]

    search = RandomizedSearchCV(
        estimator=model,
        param_distributions=param_grid,
        n_iter=10,  # increase to 20+ for deeper tuning
        scoring='f1',
        cv=3,
        verbose=1,
        random_state=42,
        n_jobs=-1
    )

    start = time.time()
    search.fit(X_train_split, y_train_split)
    end = time.time()
    duration = (end - start) / 60

    best_models[model_name] = search.best_estimator_

    print(f"‚úÖ Best Params for {model_name}: {search.best_params_}")
    print(f"üèÅ Best CV F1 Score: {search.best_score_:.4f}")
    print(f"‚è±Ô∏è Time Taken: {duration:.2f} minutes")

    # Evaluate on validation set
    y_pred = search.best_estimator_.predict(X_val_split)
    acc = accuracy_score(y_val_split, y_pred)
    prec = precision_score(y_val_split, y_pred)
    rec = recall_score(y_val_split, y_pred)
    f1 = f1_score(y_val_split, y_pred)
    cm = confusion_matrix(y_val_split, y_pred)

    results.append({
        "Model": model_name,
        "Best Params": search.best_params_,
        "CV F1": round(search.best_score_, 4),
        "Val Accuracy": round(acc, 4),
        "Val Precision": round(prec, 4),
        "Val Recall": round(rec, 4),
        "Val F1": round(f1, 4),
        "Confusion Matrix": cm.tolist(),
        "Train Time (min)": round(duration, 2)
    })

# ======================================
# üìä Step 5: Compare & Save Best Model
# ======================================
results_df = pd.DataFrame(results).sort_values(by="Val F1", ascending=False)
print("\nüìä Tuned Model Results (sorted by Validation F1):")
display(results_df)

# Identify best model
best_model_row = results_df.iloc[0]
best_model_name = best_model_row["Model"]
best_model = best_models[best_model_name]
best_params = best_model_row["Best Params"]

print(f"\nüèÖ Best Model: {best_model_name}")
print(f"‚úÖ Best Validation F1: {best_model_row['Val F1']}")
print(f"‚öôÔ∏è Best Params: {best_params}")

import time
import pandas as pd
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from catboost import CatBoostClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.ensemble import GradientBoostingClassifier

# ======================================
# ‚öôÔ∏è Step 6: Train Final Models (CPU)
# ======================================

def evaluate_model(name, model, X_train, y_train, X_val, y_val):
    """Train, evaluate, and print metrics."""
    print(f"\nüöÄ Training {name} model...")
    start = time.time()
    model.fit(X_train, y_train)
    end = time.time()

    print(f"‚úÖ {name} training complete! Time taken: {(end - start)/60:.2f} minutes")

    # Predictions
    y_pred = model.predict(X_val)

    # Metrics
    accuracy = accuracy_score(y_val, y_pred)
    precision = precision_score(y_val, y_pred)
    recall = recall_score(y_val, y_pred)
    f1 = f1_score(y_val, y_pred)
    cm = confusion_matrix(y_val, y_pred)

    print(f"\nüìà {name} Evaluation Metrics:")
    print(f"Accuracy:  {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall:    {recall:.4f}")
    print(f"F1-score:  {f1:.4f}")
    print(f"Confusion Matrix:\n{cm}")
    print("=" * 60)

    return model, f1


# ======================================
# üèÜ Best Params (from your tuning results)
# ======================================

best_params = {
    'LightGBM': {
        'subsample': 1.0,
        'num_leaves': 127,
        'n_estimators': 400,
        'max_depth': 10,
        'learning_rate': 0.1,
        'colsample_bytree': 0.8,
        'device_type': 'cpu',
        'random_state': 42,
        'n_jobs': -1
    },
    'XGBoost': {
        'subsample': 1.0,
        'n_estimators': 400,
        'max_depth': 6,
        'learning_rate': 0.1,
        'colsample_bytree': 0.8,
        'random_state': 42,
        'use_label_encoder': False,
        'eval_metric': 'logloss',
        'n_jobs': -1
    },
    'CatBoost': {
        'learning_rate': 0.05,
        'l2_leaf_reg': 7,
        'iterations': 500,
        'depth': 10,
        'verbose': 0,
        'random_seed': 42
    },
    'GradientBoosting': {
        'subsample': 0.8,
        'n_estimators': 200,
        'min_samples_split': 2,
        'learning_rate': 0.1,
        'max_depth': 5,
        'random_state': 42
    }
}

# ======================================
# üß† Initialize Models
# ======================================

models = {
    'LightGBM': LGBMClassifier(**best_params['LightGBM']),
    'XGBoost': XGBClassifier(**best_params['XGBoost']),
    'CatBoost': CatBoostClassifier(**best_params['CatBoost']),
    'GradientBoosting': GradientBoostingClassifier(**best_params['GradientBoosting'])
}

# ======================================
# üéØ Train & Evaluate
# ======================================

results = {}
for name, model in models.items():
    trained_model, f1 = evaluate_model(name, model, X_train_split, y_train_split, X_val_split, y_val_split)
    results[name] = {'model': trained_model, 'f1': f1}

# ======================================
# üèÅ Compare Models
# ======================================

print("\nüèÜ Model Comparison (F1-score):")
for name, res in results.items():
    print(f"{name}: {res['f1']:.4f}")

best_model_name = max(results, key=lambda x: results[x]['f1'])
final_model = results[best_model_name]['model']

print(f"\nü•á Best model based on Validation F1-score: {best_model_name}")

# ======================================
# üß† Generate Predictions on Test Data
# ======================================

print(f"\nüß† Generating predictions on test data using {best_model_name}...")
y_test_pred = final_model.predict(X_test_preprocessed_df)

# ======================================
# üìÅ Create Submission File
# ======================================

submission = pd.DataFrame({
    'ID': test_df['ID'],
    'default': y_test_pred.astype(int)
})

# Validation
expected_rows = 39933
expected_columns = ['ID', 'default']

print("\nüîç Validating submission format...")

if submission.shape[0] != expected_rows:
    raise ValueError(f"‚ùå Expected {expected_rows} rows, but got {submission.shape[0]}.")

if list(submission.columns) != expected_columns:
    raise ValueError(f"‚ùå Expected columns {expected_columns}, but got {list(submission.columns)}.")

if submission.isnull().any().any():
    raise ValueError("‚ùå Submission file contains missing values!")

submission.to_csv("submission.csv", index=False)
print("\n‚úÖ Submission file saved successfully as 'submission.csv'")
print(f"üìä Shape: {submission.shape}")
print(f"üìà Default Value Distribution:\n{submission['default'].value_counts()}")

X_test_preprocessed_df.shape

import pandas as pd
import joblib
import time
from lightgbm import LGBMClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# ======================================
# ‚öôÔ∏è Step 6: Final Prediction and Submission (CPU)
# ======================================

# ‚úÖ Load preprocessor
print("\nüîÑ Loading preprocessor from preprocessor.pkl...")
preprocessor = joblib.load("/content/preprocessor_pipeline.pkl")

# ‚úÖ Apply preprocessing to test data
print("‚öôÔ∏è Transforming test data...")
X_test_preprocessed_df = preprocessor.transform(test_df.drop(columns=['ID']))

# ‚úÖ Make predictions
print("\nüß† Generating predictions on test data...")
y_test_pred = final_lgbm.predict(X_test_preprocessed_df)

# ======================================
# üìÅ Step 7: Create Submission File
# ======================================
submission = pd.DataFrame({
    'ID': test_df['ID'],
    'default': y_test_pred.astype(int)
})

# ‚úÖ Validation checks
expected_rows = test_df.shape[0]   # dynamically uses your actual test data length
expected_columns = ['ID', 'default']

print("\nüîç Validating submission format...")

if submission.shape[0] != expected_rows:
    raise ValueError(f"‚ùå Expected {expected_rows} rows, but got {submission.shape[0]}.")

if list(submission.columns) != expected_columns:
    raise ValueError(f"‚ùå Expected columns {expected_columns}, but got {list(submission.columns)}.")

if submission.isnull().any().any():
    raise ValueError("‚ùå Submission file contains missing values!")

# ‚úÖ Save submission
submission.to_csv("submission.csv", index=False)
print("\n‚úÖ Submission file saved successfully as 'submission.csv'")
print(f"üìä Shape: {submission.shape}")
print(f"üìà Default Value Distribution:\n{submission['default'].value_counts()}")

submission_df = pd.read_csv("/content/submission.csv")
submission_df.shape